{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#libraries for RNN\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.models import *\n",
    "from keras.optimizers import Adam\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import datetime\n",
    "\n",
    "# mysql connection to fetch database name and table name\n",
    "\n",
    "connection_string = 'xxxx'\n",
    "engine = create_engine(connection_string, echo=False)\n",
    "conn = engine.connect()\n",
    "db =[]\n",
    "table =[]\n",
    "dbrelational= []\n",
    "\n",
    "result = conn.execute(\"SELECT * FROM xxxx\")\n",
    "for row in result:\n",
    "    db.append(row['xxxx'])\n",
    "    table.append(row['xxxx'])\n",
    "    dbrelational.append(row['xxxx'])\n",
    "conn.close()\n",
    "\n",
    "print(db)\n",
    "print(table)\n",
    "\n",
    "projectid = \"xxxx\"\n",
    "\n",
    "today = date.today( )\n",
    "yesterday = today - timedelta(days=1)\n",
    "\n",
    "for i in range(len(db)):\n",
    "    if table[i] == \"xxxx\":\n",
    "\n",
    "        file_path = db[i]+\"_model.h5\"\n",
    "    \n",
    "        if os.path.exists(file_path):\n",
    "        \n",
    "            ## checking age of model first before we proceed!\n",
    "            today1 = datetime.datetime.now() \n",
    "            modeldate = datetime.datetime.strptime(open(db[i]+'_modeltime.txt',\"r\").read().replace('\\n','')  , '%Y-%m-%d %H:%M:%S')\n",
    "            model_age = today1 - modeldate\n",
    "            age_of_model = int(str(model_age.days))\n",
    "            print(\"Age of model\", age_of_model)\n",
    "        \n",
    "            if age_of_model <5:\n",
    "            \n",
    "                print(\"Age of model is less than 5\")\n",
    "                data = pd.read_gbq('SELECT DATE, ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign, SUM(ga_sessions)AS ga_sessions, SUM(ga_bounces)/SUM(ga_sessions) AS ga_bounceRate, SUM( ga_transactions)/SUM(ga_sessions) AS ga_conversionRate FROM '+db[i]+'.'+table[i]+' group by DATE,ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign', projectid)\n",
    "                mask=(data['DATE'] >=yesterday) & (data['DATE'] < today)\n",
    "                data = data.loc[mask]\n",
    "                \n",
    "                # the day of the week (Monday=0, Sunday=6)\n",
    "                data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "                data['DayOfTheWeek'] = data['DATE'].dt.dayofweek\n",
    "                # time with int to plot easily\n",
    "                data['time_epoch'] = (data['DATE'].astype(np.int64)/100000000000).astype(np.int64)\n",
    "                data.fillna(value=0, inplace=True)\n",
    "\n",
    "                ## to convert categorical to numeric and saving the mapping for reverse mapping in the end\n",
    "                data['ga_deviceCategory'] = data['ga_deviceCategory'].astype('category')\n",
    "                d_ga_deviceCategory = dict(enumerate(data['ga_deviceCategory'].cat.categories))\n",
    "                data['ga_deviceCategory_codes'] = data['ga_deviceCategory'].cat.codes\n",
    "\n",
    "                data['ga_channelgrouping'] = data['ga_channelgrouping'].astype('category')\n",
    "                d_ga_channelgrouping = dict(enumerate(data['ga_channelgrouping'].cat.categories))\n",
    "                data['ga_channelgrouping_codes'] = data['ga_channelgrouping'].cat.codes\n",
    "\n",
    "                data['ga_sourceMedium'] = data['ga_sourceMedium'].astype('category')\n",
    "                d_ga_sourceMedium = dict(enumerate(data['ga_sourceMedium'].cat.categories))\n",
    "                data['ga_sourceMedium_codes'] = data['ga_sourceMedium'].cat.codes\n",
    "\n",
    "                data['ga_campaign'] = data['ga_campaign'].astype('category')\n",
    "                d_ga_campaign = dict(enumerate(data['ga_campaign'].cat.categories))\n",
    "                data['ga_campaign_codes'] = data['ga_campaign'].cat.codes\n",
    "\n",
    "                #select and standardize data\n",
    "                data_set = data[['ga_bounceRate', 'ga_sessions','DayOfTheWeek','time_epoch','ga_deviceCategory_codes','ga_channelgrouping_codes','ga_sourceMedium_codes','ga_campaign_codes','ga_conversionRate']]\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                np_scaled = min_max_scaler.fit_transform(data_set)\n",
    "                data_set = pd.DataFrame(np_scaled)\n",
    "\n",
    "                #important parameters and train/test size\n",
    "                look_back = 6\n",
    "                traindatasize = 0\n",
    "\n",
    "                x_test = data_set.iloc[traindatasize::,0:9].as_matrix()\n",
    "                y_1_test=data_set.iloc[traindatasize::,0].as_matrix() ## ga_bounceRate\n",
    "                y_2_test=data_set.iloc[traindatasize::,1].as_matrix() ## ga_sessions\n",
    "\n",
    "                print(\"x_test shape\",x_test.shape)\n",
    "                print(\"y_1_test shape\",y_1_test.shape)\n",
    "                print(\"y_2_test shape\",y_2_test.shape)\n",
    "\n",
    "                def load_data(data,sequence_length):\n",
    "                    result = []\n",
    "                    for index in range(len(data) - sequence_length):\n",
    "                        result.append(data[index: index + sequence_length])\n",
    "                    return np.asarray(result)\n",
    "\n",
    "                x_test  = load_data(x_test,look_back)\n",
    "                y_11_test  = y_1_test[-x_test.shape[0]:]\n",
    "                y_22_test  = y_2_test[-x_test.shape[0]:]\n",
    "\n",
    "                print(\"\")\n",
    "                print(\"x_test \", x_test.shape)\n",
    "                print(\"y_11_test\", y_11_test.shape)\n",
    "                print(\"y_22_test\", y_22_test.shape)\n",
    "\n",
    "                # To find the H.C.F of two input number for batch_size -> might be unnecessary\n",
    "                \n",
    "                def computeHCF(x, y):\n",
    "                    \n",
    "                # choose the smaller number\n",
    "                    if x > y:\n",
    "                        smaller = y\n",
    "                    else:\n",
    "                        smaller = x\n",
    "                    for i in range(1, smaller+1):\n",
    "                        if((x % i == 0) and (y % i == 0)):\n",
    "                            hcf = i   \n",
    "                    return hcf\n",
    "                \n",
    "                batch_size= computeHCF(x_test.shape[0], x_test.shape[0])\n",
    "                print(\"batch size: \", batch_size)\n",
    "                \n",
    "                ## defining the model\n",
    "                def my_model():\n",
    "                    \n",
    "                    input_x = Input(batch_shape=(batch_size, look_back, x_test.shape[2]), name='input')\n",
    "                    drop = Dropout(0.5)\n",
    "                    \n",
    "                    lstm_1, state_h, state_c = LSTM(128, return_sequences=False, batch_input_shape=(batch_size, look_back, x_test.shape[2]), name='3dLSTM', return_state=True)(input_x)\n",
    "                    lstm_1_drop = drop(lstm_1)\n",
    "                    \n",
    "                    y1 = Dense(1, activation='sigmoid', name='op1')(lstm_1_drop)\n",
    "                    y2 = Dense(1, activation='sigmoid', name='op2')(lstm_1_drop)\n",
    "                    \n",
    "                    model1 = Model(input=[input_x],output=[y1,y2])\n",
    "                    model2 = Model(input=[input_x],output=[y1,y2,state_h,state_c])\n",
    "\n",
    "                    optimizer = Adam(lr=0.0005, decay=0.00001)\n",
    "                    model1.compile(loss='mse', optimizer=optimizer,metrics=['mse'])\n",
    "\n",
    "                    model1.summary()\n",
    "                    model2.summary()\n",
    "                    \n",
    "                    return model1, model2\n",
    "\n",
    "                model1,model2 = my_model()\n",
    "\n",
    "\n",
    "                ## load the model .h5 file\n",
    "                model2.load_weights(db[i]+\"_model.h5\")\n",
    "                \n",
    "                print(db[i]+\"_model.h5\", \"Model found\")\n",
    "                p = model2.predict(x_test, batch_size=batch_size)\n",
    "#                 print(np.asarray(p).shape)\n",
    "\n",
    "                diff1=[]\n",
    "                diff2=[]\n",
    "\n",
    "                for u in range(len(y_11_test)):\n",
    "                    pr1 = p[0][u]\n",
    "                    diff1.append(abs(y_11_test[u]- pr1))\n",
    "\n",
    "                for u in range(len(y_22_test)):\n",
    "                    pr2 = p[1][u]\n",
    "                    diff2.append(abs(y_22_test[u]- pr2))\n",
    "                 \n",
    "                print(\"\")\n",
    "                sh = data.shape[0]\n",
    "                \n",
    "                if 0 <= sh <= 100:\n",
    "                    contamination_factor =0.1\n",
    "                if 101 <= sh <= 1000:\n",
    "                    contamination_factor =0.01\n",
    "                if 1001 <= sh <= 10000:\n",
    "                    contamination_factor =0.001\n",
    "                if 10001 <= sh <= 100000:\n",
    "                    contamination_factor =0.0001\n",
    "                if 100001 <= sh <= 1000000:\n",
    "                    contamination_factor =0.00001\n",
    "                    \n",
    "                print(\"Using contamination factor of \", contamination_factor)\n",
    "                print(\" \")\n",
    "                \n",
    "                \n",
    "                diff1 = pd.Series(diff1)\n",
    "                number_of_outliers1 = int(contamination_factor*len(diff1))\n",
    "                print(\"No of Outliers for ga_bounceRate duration: \", number_of_outliers1)\n",
    "\n",
    "                diff1=diff1.astype('float64')\n",
    "                threshold1 = diff1.nlargest(number_of_outliers1).min()\n",
    "                print(\"Threshold value for ga_bounceRate: \", threshold1)\n",
    "\n",
    "                ## data with anomaly label (test data part)\n",
    "                test1 = (diff1 > threshold1).astype(int)\n",
    "                print(\"Shape of Test1 data\", test1.shape)\n",
    "\n",
    "                diff2 = pd.Series(diff2)\n",
    "                number_of_outliers2 = int(contamination_factor*len(diff2))\n",
    "                print(\"No of Outliers in sessions: \",number_of_outliers2)\n",
    "\n",
    "                diff2=diff2.astype('float64')\n",
    "                threshold2 = diff2.nlargest(number_of_outliers2).min()\n",
    "                print(\"Threshold value for sessions\", threshold2)\n",
    "                # data with anomaly label (test data part)\n",
    "                test2 = (diff2 > threshold2).astype(int)\n",
    "\n",
    "                print(\"Shape of test2 data\",test2.shape)\n",
    "                data_set_test = data_set.iloc[traindatasize::]\n",
    "\n",
    "                # data_set_test.fillna(value=0, inplace=True)\n",
    "                inversed_test_df = min_max_scaler.inverse_transform(data_set_test)\n",
    "                df2 = pd.DataFrame(inversed_test_df)\n",
    "                df2.columns = ['ga_bounceRate', 'ga_sessions','DayOfTheWeek','time_epoch','ga_deviceCategory_codes','ga_channelgrouping_codes','ga_sourceMedium_codes','ga_campaign_codes','ga_conversionRate']\n",
    "\n",
    "                ## predictions to dataframe\n",
    "                pred_resultsbounceRate = pd.DataFrame(p[0])## bounceRate\n",
    "                pred_resultsSessions = pd.DataFrame(p[1])## anomalySessions\n",
    "               \n",
    "                # data_set = data_set.fillna(value=0, inplace=True)\n",
    "                pred_Results = pd.concat([pred_resultsbounceRate, pred_resultsSessions], axis=1)\n",
    "                pred_Results_DF = pd.concat([pred_Results,df2.iloc[:,2:9] ], axis=1)\n",
    "\n",
    "                pred_Results_DF.fillna(value=0, inplace=True)\n",
    "                inversed_x_test_pred_df = min_max_scaler.inverse_transform(pred_Results_DF)\n",
    "                df1 = pd.DataFrame(inversed_x_test_pred_df)\n",
    "                df1.columns = ['Predict_ga_bounceRate', 'Predict_ga_sessions','DayOfTheWeek','time_epoch','ga_deviceCategory_codes','ga_channelgrouping_codes','ga_sourceMedium_codes','ga_campaign_codes','ga_conversionRate']\n",
    "\n",
    "                pred_df = df1.iloc[:,0:2]\n",
    "                merge_df = pd.concat([pred_df, df2], axis=1, join='inner')\n",
    "\n",
    "                complement = pd.Series()\n",
    "                ## add the data to the main\n",
    "                merge_df['IsAnomalybounceRate'] = complement.append(test1, ignore_index='True')\n",
    "                print(merge_df['IsAnomalybounceRate'].value_counts())\n",
    "\n",
    "                ## add the data to the main\n",
    "                merge_df['IsAnomalysessions'] = complement.append(test2, ignore_index='True')\n",
    "                print(merge_df['IsAnomalysessions'].value_counts())\n",
    "\n",
    "                AnomalybounceRate = merge_df.loc[merge_df['IsAnomalybounceRate']  == 1]\n",
    "                AnomalySessions = merge_df.loc[merge_df['IsAnomalysessions']  == 1]\n",
    "                result = pd.concat([AnomalybounceRate, AnomalySessions], axis=0, join='inner')            \n",
    "           \n",
    "                # sending an empty dataframe with same columns to sql- if result is empty\n",
    "                if result.empty:\n",
    "                    print('DataFrame is empty!')\n",
    "                \n",
    "                    emptyframe = pd.DataFrame(columns=['DATE','Predict_ga_bounceRate', 'Predict_ga_sessions', 'ga_bounceRate',\n",
    "                                        'ga_sessions', 'DayOfTheWeek', 'time_epoch', 'ga_deviceCategory_reversed',\n",
    "                                        'ga_channelgrouping_reversed', 'ga_sourceMedium_reversed',\n",
    "                                        'ga_campaign_reversed', 'ga_conversionRate', 'IsAnomalybounceRate',\n",
    "                                        'IsAnomalysessions'] )\n",
    "                \n",
    "                    connection_string= 'mysql+mysqlconnector://romi:password@35.205.244.6:3306/'+dbrelational[i]\n",
    "                    engine = create_engine(connection_string, echo=False)\n",
    "                    conn = engine.connect()\n",
    "                    emptyframe.to_sql(name=\"GA_anomalyDetection\", con=engine, if_exists='replace', chunksize=100,index_label='Id')\n",
    "                    print(\"Empty dataframe pushed to Sql!\")\n",
    "                    \n",
    "                else:\n",
    "                    ## need to put these 5 line in the else section to do this operation before pushing to sql\n",
    "                    result['ga_deviceCategory_reversed'] = result['ga_deviceCategory_codes'].map(d_ga_deviceCategory)\n",
    "                    result['ga_channelgrouping_reversed'] = result['ga_channelgrouping_codes'].map(d_ga_channelgrouping)\n",
    "                    result['ga_sourceMedium_reversed'] = result['ga_sourceMedium_codes'].map(d_ga_sourceMedium)\n",
    "                    result['ga_campaign_reversed'] = result['ga_campaign_codes'].map(d_ga_campaign)\n",
    "\n",
    "                    finalresult = result[['Predict_ga_bounceRate', 'Predict_ga_sessions', 'ga_bounceRate',\n",
    "                        'ga_sessions', 'DayOfTheWeek', 'time_epoch', 'ga_deviceCategory_reversed',\n",
    "                        'ga_channelgrouping_reversed', 'ga_sourceMedium_reversed',\n",
    "                        'ga_campaign_reversed', 'ga_conversionRate', 'IsAnomalybounceRate',\n",
    "                        'IsAnomalysessions']]\n",
    "                    \n",
    "                    dates =[]\n",
    "\n",
    "                    for j in finalresult[\"time_epoch\"]:\n",
    "                        dates.append(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(j*100)))\n",
    "\n",
    "                    df_dates = pd.DataFrame({'DATE':dates})\n",
    "                    totalfinalresult = pd.DataFrame(np.hstack([df_dates,finalresult]))\n",
    "                    totalfinalresult.columns = ['DATE','Predict_ga_bounceRate', 'Predict_ga_sessions', 'ga_bounceRate',\n",
    "                                        'ga_sessions', 'DayOfTheWeek', 'time_epoch', 'ga_deviceCategory_reversed',\n",
    "                                        'ga_channelgrouping_reversed', 'ga_sourceMedium_reversed',\n",
    "                                        'ga_campaign_reversed', 'ga_conversionRate', 'IsAnomalybounceRate',\n",
    "                                        'IsAnomalysessions','Id']\n",
    "\n",
    "                    ## outputting the final dataframe to mysql DB of respective clients\n",
    "                    connection_string= 'mysql+mysqlconnector://romi:password@35.205.244.6:3306/'+dbrelational[i]\n",
    "                    engine = create_engine(connection_string, echo=False)\n",
    "                    conn = engine.connect()\n",
    "                    totalfinalresult.to_sql(name=\"GA_anomalyDetection\", con=engine, if_exists='replace', chunksize=100, index_label='Id')\n",
    "                    print(\"Saved to mysql\")\n",
    "\n",
    "            elif age_of_model >= 5:\n",
    "\n",
    "                print(\"Age of model is more than or equal to 5\")\n",
    "                data = pd.read_gbq('SELECT DATE, ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign, SUM(ga_sessions)AS ga_sessions, SUM(ga_bounces)/SUM(ga_sessions) AS ga_bounceRate, SUM( ga_transactions)/SUM(ga_sessions) AS ga_conversionRate FROM '+db[i]+'.'+table[i]+' group by DATE,ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign', projectid)\n",
    "                today = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "                f = open(db[i]+'_modeltime.txt',\"r\")\n",
    "                \n",
    "                last_date = f.read()\n",
    "                f.close()\n",
    "\n",
    "                mask=(data['DATE'] >last_date) & (data['DATE'] <= today)\n",
    "                data = data.loc[mask]\n",
    "\n",
    "                print(\"Trainig model from :\", last_date, \" to \", today)\n",
    "                print(\"Training model for databse: \"+db[i])\n",
    "\n",
    "                # the day of the week (Monday=0, Sunday=6)\n",
    "                data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "                data['DayOfTheWeek'] = data['DATE'].dt.dayofweek\n",
    "                # time with int to plot easily\n",
    "                data['time_epoch'] = (data['DATE'].astype(np.int64)/100000000000).astype(np.int64)\n",
    "                data.fillna(value=0, inplace=True)\n",
    "\n",
    "                ## to convert categorical to numeric and saving the mapping for reverse mapping in the end\n",
    "                data['ga_deviceCategory'] = data['ga_deviceCategory'].astype('category')\n",
    "                d_ga_deviceCategory = dict(enumerate(data['ga_deviceCategory'].cat.categories))\n",
    "                data['ga_deviceCategory_codes'] = data['ga_deviceCategory'].cat.codes\n",
    "\n",
    "                data['ga_channelgrouping'] = data['ga_channelgrouping'].astype('category')\n",
    "                d_ga_channelgrouping = dict(enumerate(data['ga_channelgrouping'].cat.categories))\n",
    "                data['ga_channelgrouping_codes'] = data['ga_channelgrouping'].cat.codes\n",
    "\n",
    "                data['ga_sourceMedium'] = data['ga_sourceMedium'].astype('category')\n",
    "                d_ga_sourceMedium = dict(enumerate(data['ga_sourceMedium'].cat.categories))\n",
    "                data['ga_sourceMedium_codes'] = data['ga_sourceMedium'].cat.codes\n",
    "\n",
    "                data['ga_campaign'] = data['ga_campaign'].astype('category')\n",
    "                d_ga_campaign = dict(enumerate(data['ga_campaign'].cat.categories))\n",
    "                data['ga_campaign_codes'] = data['ga_campaign'].cat.codes\n",
    "\n",
    "                #select and standardize data\n",
    "                data_set = data[['ga_bounceRate', 'ga_sessions','DayOfTheWeek','time_epoch','ga_deviceCategory_codes','ga_channelgrouping_codes','ga_sourceMedium_codes','ga_campaign_codes','ga_conversionRate']]\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                np_scaled = min_max_scaler.fit_transform(data_set)\n",
    "                data_set = pd.DataFrame(np_scaled)\n",
    "\n",
    "                #important parameters and train/test size\n",
    "                look_back = 6\n",
    "                traindatasize = int(len(data_set) * 0.90)\n",
    "\n",
    "                x_train = data_set.iloc[0:traindatasize:,0:9].as_matrix()\n",
    "                y_1_train=data_set.iloc[0:traindatasize:,0].as_matrix() ## ga_bounceRate\n",
    "                y_2_train=data_set.iloc[0:traindatasize:,1].as_matrix() ## ga_sessions\n",
    "\n",
    "                x_test = data_set.iloc[traindatasize::,0:9].as_matrix()\n",
    "                y_1_test=data_set.iloc[traindatasize::,0].as_matrix() ## ga_bounceRate\n",
    "                y_2_test=data_set.iloc[traindatasize::,1].as_matrix() ## ga_sessions\n",
    "\n",
    "                print(\"Before including look back load data\")\n",
    "                print(\"\")\n",
    "                print(\"x_train shape \",x_train.shape)\n",
    "                print(\"y_1_train shape \",y_1_train.shape)\n",
    "                print(\"y_2_train shape \",y_2_train.shape)\n",
    "\n",
    "                print(\"x_test shape\",x_test.shape)\n",
    "                print(\"y_1_test shape\",y_1_test.shape)\n",
    "                print(\"y_2_test shape\",y_2_test.shape)\n",
    "\n",
    "                def load_data(data,sequence_length):\n",
    "                    result = []\n",
    "                    for index in range(len(data) - sequence_length):\n",
    "                        result.append(data[index: index + sequence_length])\n",
    "                    return np.asarray(result)\n",
    "\n",
    "                # adapt the datasets for the sequence data shape\n",
    "                x_train = load_data(x_train,look_back)\n",
    "                y_11_train = y_1_train[-x_train.shape[0]:]\n",
    "                y_22_train = y_2_train[-x_train.shape[0]:]\n",
    "\n",
    "                x_test  = load_data(x_test,look_back)\n",
    "                y_11_test  = y_1_test[-x_test.shape[0]:]\n",
    "                y_22_test  = y_2_test[-x_test.shape[0]:]\n",
    "\n",
    "                print(\"\")  \n",
    "                print(\"After look back load data\")\n",
    "                print(\"\")\n",
    "                print(\"x_train \", x_train.shape)\n",
    "                print(\"y_11_train\", y_11_train.shape)\n",
    "                print(\"y_22_train\", y_22_train.shape)\n",
    "\n",
    "                print(\"\")\n",
    "                print(\"x_test \", x_test.shape)\n",
    "                print(\"y_11_test\", y_11_test.shape)\n",
    "                print(\"y_22_test\", y_22_test.shape)\n",
    "\n",
    "                # To find the H.C.F of two input number for batch_size -> might be unnecessary\n",
    "                def computeHCF(x, y):\n",
    "                # choose the smaller number\n",
    "                    if x > y:\n",
    "                        smaller = y\n",
    "                    else:\n",
    "                        smaller = x\n",
    "                    for i in range(1, smaller+1):\n",
    "                        if((x % i == 0) and (y % i == 0)):\n",
    "                            hcf = i\n",
    "\n",
    "                    return hcf\n",
    "           \n",
    "                batch_size= computeHCF(x_train.shape[0], x_test.shape[0])\n",
    "                print(\"batch size: \", batch_size)\n",
    "\n",
    "                ## defining the model\n",
    "                def my_model():\n",
    "                    input_x = Input(batch_shape=(batch_size, look_back, x_test.shape[2]), name='input')\n",
    "                    drop = Dropout(0.5)\n",
    "\n",
    "                    lstm_1, state_h, state_c = LSTM(128, return_sequences=False, batch_input_shape=(batch_size, look_back, x_test.shape[2]), name='3dLSTM', return_state=True)(input_x)\n",
    "                    lstm_1_drop = drop(lstm_1)\n",
    "\n",
    "                    y1 = Dense(1, activation='sigmoid', name='op1')(lstm_1_drop)\n",
    "                    y2 = Dense(1, activation='sigmoid', name='op2')(lstm_1_drop)\n",
    "\n",
    "                    model1 = Model(input=[input_x],output=[y1,y2])\n",
    "                    model2 = Model(input=[input_x],output=[y1,y2,state_h,state_c])\n",
    "\n",
    "                    optimizer = Adam(lr=0.0005, decay=0.00001)\n",
    "                    model1.compile(loss='mse', optimizer=optimizer,metrics=['mse'])\n",
    "                    \n",
    "                    model1.summary()\n",
    "                    model2.summary()\n",
    "\n",
    "                    return model1, model2\n",
    "\n",
    "                model1,model2 = my_model()\n",
    "                model1.load_weights(db[i]+\"_model.h5\")\n",
    "                history = model1.fit(x_train, [y_11_train,y_22_train], epochs=100, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\n",
    "                # serialize weights to HDF5 and save the model\n",
    "                model1.save_weights(db[i]+\"_model.h5\")\n",
    "                print(\"Updated Model saved to disk. Model name: \",db[i]+\"_model.h5\")\n",
    "            \n",
    "                with open(db[i]+'_modeltime.txt', 'w+') as f:\n",
    "                    print(str(datetime.datetime.now().replace(microsecond=0)), file=f)\n",
    "        else:\n",
    "            data = pd.read_gbq('SELECT DATE, ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign, SUM(ga_sessions)AS ga_sessions, SUM(ga_bounces)/SUM(ga_sessions) AS ga_bounceRate, SUM( ga_transactions)/SUM(ga_sessions) AS ga_conversionRate FROM '+db[i]+'.'+table[i]+' group by DATE,ga_deviceCategory, ga_channelgrouping, ga_sourceMedium, ga_campaign', projectid)\n",
    "            print(\"Training Model for database:\"+db[i])\n",
    "            data.fillna(0,inplace=True)\n",
    "            print(\"Fetched data shape: \",data.shape)\n",
    "            \n",
    "            data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "            data['DayOfTheWeek'] = data['DATE'].dt.dayofweek\n",
    "            # time with int to plot easily\n",
    "            data['time_epoch'] = (data['DATE'].astype(np.int64)/100000000000).astype(np.int64)\n",
    "            data.fillna(value=0, inplace=True)\n",
    "\n",
    "            ## to convert categorical to numeric and saving the mapping for reverse mapping in the end\n",
    "            data['ga_deviceCategory'] = data['ga_deviceCategory'].astype('category')\n",
    "            d_ga_deviceCategory = dict(enumerate(data['ga_deviceCategory'].cat.categories))\n",
    "            data['ga_deviceCategory_codes'] = data['ga_deviceCategory'].cat.codes\n",
    "\n",
    "            data['ga_channelgrouping'] = data['ga_channelgrouping'].astype('category')\n",
    "            d_ga_channelgrouping = dict(enumerate(data['ga_channelgrouping'].cat.categories))\n",
    "            data['ga_channelgrouping_codes'] = data['ga_channelgrouping'].cat.codes\n",
    "\n",
    "            data['ga_sourceMedium'] = data['ga_sourceMedium'].astype('category')\n",
    "            d_ga_sourceMedium = dict(enumerate(data['ga_sourceMedium'].cat.categories))\n",
    "            data['ga_sourceMedium_codes'] = data['ga_sourceMedium'].cat.codes\n",
    "\n",
    "            data['ga_campaign'] = data['ga_campaign'].astype('category')\n",
    "            d_ga_campaign = dict(enumerate(data['ga_campaign'].cat.categories))\n",
    "            data['ga_campaign_codes'] = data['ga_campaign'].cat.codes\n",
    "\n",
    "            #select and standardize data\n",
    "            data_set = data[['ga_bounceRate', 'ga_sessions','DayOfTheWeek','time_epoch','ga_deviceCategory_codes','ga_channelgrouping_codes','ga_sourceMedium_codes','ga_campaign_codes','ga_conversionRate']]\n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            np_scaled = min_max_scaler.fit_transform(data_set)\n",
    "            data_set = pd.DataFrame(np_scaled)\n",
    "\n",
    "            #important parameters and train/test size \n",
    "            look_back = 6\n",
    "            traindatasize = int(len(data_set) * 0.90)\n",
    "\n",
    "            x_train = data_set.iloc[0:traindatasize:,0:9].as_matrix()\n",
    "            y_1_train=data_set.iloc[0:traindatasize:,0].as_matrix() ## ga_bounceRate\n",
    "            y_2_train=data_set.iloc[0:traindatasize:,1].as_matrix() ## ga_sessions\n",
    "\n",
    "            x_test = data_set.iloc[traindatasize::,0:9].as_matrix()\n",
    "            y_1_test=data_set.iloc[traindatasize::,0].as_matrix() ## ga_bounceRate\n",
    "            y_2_test=data_set.iloc[traindatasize::,1].as_matrix() ## ga_sessions\n",
    "\n",
    "            def load_data(data,sequence_length):\n",
    "                result = []\n",
    "                for index in range(len(data) - sequence_length):\n",
    "                    result.append(data[index: index + sequence_length])\n",
    "                return np.asarray(result)\n",
    "\n",
    "            # adapt the datasets for the sequence data shape\n",
    "            x_train = load_data(x_train,look_back)\n",
    "            y_11_train = y_1_train[-x_train.shape[0]:]\n",
    "            y_22_train = y_2_train[-x_train.shape[0]:]\n",
    "\n",
    "            x_test  = load_data(x_test,look_back)\n",
    "            y_11_test  = y_1_test[-x_test.shape[0]:]\n",
    "            y_22_test  = y_2_test[-x_test.shape[0]:]\n",
    "\n",
    "            print(\"\")  \n",
    "            print(\"After look back load data\")\n",
    "            print(\"\")\n",
    "            print(\"x_train \", x_train.shape)\n",
    "            print(\"y_11_train\", y_11_train.shape)\n",
    "            print(\"y_22_train\", y_22_train.shape)\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"x_test \", x_test.shape)\n",
    "            print(\"y_11_test\", y_11_test.shape)\n",
    "            print(\"y_22_test\", y_22_test.shape)\n",
    "\n",
    "            # To find the H.C.F of two input number for batch_size\n",
    "            def computeHCF(x, y):\n",
    "            # choose the smaller number\n",
    "                if x > y:\n",
    "                    smaller = y\n",
    "                else:\n",
    "                    smaller = x\n",
    "                for i in range(1, smaller+1):\n",
    "                    if((x % i == 0) and (y % i == 0)):\n",
    "                        hcf = i\n",
    "\n",
    "                return hcf\n",
    "\n",
    "            batch_size= computeHCF(x_train.shape[0], x_test.shape[0])\n",
    "            print(\"batch size: \", batch_size)\n",
    "\n",
    "            ## defining the model\n",
    "            def my_model():\n",
    "\n",
    "                input_x = Input(batch_shape=(batch_size, look_back, x_test.shape[2]), name='input')\n",
    "                drop = Dropout(0.5)\n",
    "\n",
    "                lstm_1, state_h, state_c = LSTM(128, return_sequences=False, batch_input_shape=(batch_size, look_back, x_test.shape[2]), name='3dLSTM', return_state=True)(input_x)\n",
    "                lstm_1_drop = drop(lstm_1)\n",
    "\n",
    "                y1 = Dense(1, activation='sigmoid', name='op1')(lstm_1_drop)\n",
    "                y2 = Dense(1, activation='sigmoid', name='op2')(lstm_1_drop)\n",
    "\n",
    "                model1 = Model(input=[input_x],output=[y1,y2])\n",
    "                model2 = Model(input=[input_x],output=[y1,y2,state_h,state_c])\n",
    "\n",
    "                optimizer = Adam(lr=0.0005, decay=0.00001)\n",
    "                model1.compile(loss='mse', optimizer=optimizer,metrics=['mse'])\n",
    "                \n",
    "                model1.summary()\n",
    "                model2.summary()\n",
    "                \n",
    "                return model1, model2\n",
    "            model1,model2 = my_model()\n",
    "        \n",
    "            print(\"Training started...\")\n",
    "            print(\"\")\n",
    "            history = model1.fit(x_train, [y_11_train,y_22_train], epochs=100, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "            # serialize weights to HDF5 and save the model\n",
    "            model1.save_weights(db[i]+\"_model.h5\")\n",
    "            print(\"Saved model to disk. Model name: \", db[i]+\"_model.h5\")\n",
    "            \n",
    "            with open(db[i]+'_modeltime.txt', 'w+') as f:\n",
    "                print(str(datetime.datetime.now().replace(microsecond=0)), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
